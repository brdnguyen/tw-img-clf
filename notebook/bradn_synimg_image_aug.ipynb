{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36msynimg\u001b[m\u001b[m/\n",
      "train-Beijing-1050000.jpg\n",
      "train-Beijing-1050001.jpg\n",
      "train-Beijing-1050002.jpg\n",
      "train-Beijing-1050003.jpg\n",
      "train-Beijing-1050004.jpg\n"
     ]
    }
   ],
   "source": [
    "%ls ../input\n",
    "%ls ../input/synimg/synimg/train/Beijing | head -5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import skimage.io as io\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../src\")  # This is relative to test directory e.g. /realestatereview/test\n",
    "from data import read_data_from_file, get_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 64\n",
    "HEIGHT = 32\n",
    "ROOT_PATH = '../input/synimg/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 1: Doing input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id style_name                      file  \\\n",
      "0  1000000     Luanda  train-Luanda-1000000.jpg   \n",
      "\n",
      "                                       filepath  \n",
      "0  synimg/train/Luanda/train-Luanda-1000000.jpg  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>style_name</th>\n",
       "      <th>file</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000000.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000001.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000002.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000003.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000004.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000005</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000005.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000006</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000006.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000006.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000007</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000007.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000007.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000008</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000008.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000008.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000009</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>train-Luanda-1000009.jpg</td>\n",
       "      <td>synimg/train/Luanda/train-Luanda-1000009.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id style_name                      file  \\\n",
       "0  1000000     Luanda  train-Luanda-1000000.jpg   \n",
       "1  1000001     Luanda  train-Luanda-1000001.jpg   \n",
       "2  1000002     Luanda  train-Luanda-1000002.jpg   \n",
       "3  1000003     Luanda  train-Luanda-1000003.jpg   \n",
       "4  1000004     Luanda  train-Luanda-1000004.jpg   \n",
       "5  1000005     Luanda  train-Luanda-1000005.jpg   \n",
       "6  1000006     Luanda  train-Luanda-1000006.jpg   \n",
       "7  1000007     Luanda  train-Luanda-1000007.jpg   \n",
       "8  1000008     Luanda  train-Luanda-1000008.jpg   \n",
       "9  1000009     Luanda  train-Luanda-1000009.jpg   \n",
       "\n",
       "                                       filepath  \n",
       "0  synimg/train/Luanda/train-Luanda-1000000.jpg  \n",
       "1  synimg/train/Luanda/train-Luanda-1000001.jpg  \n",
       "2  synimg/train/Luanda/train-Luanda-1000002.jpg  \n",
       "3  synimg/train/Luanda/train-Luanda-1000003.jpg  \n",
       "4  synimg/train/Luanda/train-Luanda-1000004.jpg  \n",
       "5  synimg/train/Luanda/train-Luanda-1000005.jpg  \n",
       "6  synimg/train/Luanda/train-Luanda-1000006.jpg  \n",
       "7  synimg/train/Luanda/train-Luanda-1000007.jpg  \n",
       "8  synimg/train/Luanda/train-Luanda-1000008.jpg  \n",
       "9  synimg/train/Luanda/train-Luanda-1000009.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data_from_file(filename, nrows=None, max_per_class=None, shuffle=False):\n",
    "    '''\n",
    "        File is in the format of data.csv or data_nostyle.csv,\n",
    "        containing location of each image and IDs and/or la\n",
    "        filename example: 'synimg/test/data_nostyle.csv'\n",
    "    '''\n",
    "    data = pd.read_csv(ROOT_PATH + filename, nrows=nrows)\n",
    "    if max_per_class:\n",
    "        data = data.groupby('style_name').head(max_per_class).reset_index()\n",
    "    # print(\"Shape and review after getting max per group:\\n\", data.shape, \"\\n\", data.head(20))\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1.0) \n",
    "    # Read images\n",
    "#     all_images = read_images(data, max_per_class) # includes caching\n",
    "\n",
    "    return data\n",
    "\n",
    "MAX_PER_CLASS=500\n",
    "train_data =  read_data_from_file('synimg/train/data.csv', max_per_class=None, shuffle=False)\n",
    "test_data = read_data_from_file('synimg/test/data_nostyle.csv')\n",
    "print(train_data.head(1))\n",
    "train_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def read_images(data):\n",
    "    X = []\n",
    "    for filepath in data['filepath']:\n",
    "        pil_img = Image.open(ROOT_PATH + filepath)\n",
    "        img_array = np.array(pil_img) # [0., 255.0]\n",
    "        X.append(img_array)\n",
    "    return np.stack(X)\n",
    "\n",
    "#     all_images = []\n",
    "#     for idx, row in data.iterrows():\n",
    "#         img = io.imread(ROOT_PATH + row['filepath'])\n",
    "#         all_images.append(img)\n",
    "#     return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = read_images(train_data)\n",
    "test_images = read_images(test_data)\n",
    "label_encoder, train_data = get_labels(train_data, print_classes=False) # one-hot encode, returns in column 'style_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this to shuffle data if needed\n",
    "train_data = train_data.sample(frac = 1.0)\n",
    "train_images = read_images(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's try to see an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 75 236  72]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[2])\n",
    "print(train_images[0][2][5])\n",
    "train_images[0][2][5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def normalise(images):\n",
    "    images = np.array(images).astype(np.float32)\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "#         images[idx] = (images[idx] / 255.0 * 2.0) - 1.0\n",
    "        images[idx] = (images[idx] / 255.0 )\n",
    "        \n",
    "    return images\n",
    "\n",
    "X_train, X_test = normalise(train_images), normalise(test_images)\n",
    "# y_train = keras.utils.to_categorical(list(train_data['style_id']), NUM_CLASSES) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label distribution Counter({0: 9547, 6: 9540, 3: 9519, 9: 9515, 1: 9506, 8: 9502, 4: 9488, 7: 9481, 2: 9454, 5: 9448})\n",
      "validation label distribution Counter({5: 552, 2: 546, 7: 519, 4: 512, 8: 498, 1: 494, 9: 485, 3: 481, 6: 460, 0: 453})\n",
      "X_train[0] [[[0.14901961 1.         0.21568628]\n",
      "  [0.27450982 0.93333334 0.2627451 ]\n",
      "  [0.36862746 0.8745098  0.31764707]\n",
      "  ...\n",
      "  [0.9764706  1.         0.972549  ]\n",
      "  [0.96862745 1.         0.9764706 ]\n",
      "  [0.9647059  1.         0.9843137 ]]\n",
      "\n",
      " [[0.1764706  0.9882353  0.23529412]\n",
      "  [0.26666668 0.9411765  0.26666668]\n",
      "  [0.34901962 0.89411765 0.29411766]\n",
      "  ...\n",
      "  [0.98039216 1.         0.95686275]\n",
      "  [0.9764706  1.         0.972549  ]\n",
      "  [0.9764706  1.         0.9764706 ]]\n",
      "\n",
      " [[0.22352941 0.9607843  0.28235295]\n",
      "  [0.25490198 0.94509804 0.28235295]\n",
      "  [0.32941177 0.9098039  0.28627452]\n",
      "  ...\n",
      "  [0.99215686 1.         0.94509804]\n",
      "  [0.99215686 1.         0.9490196 ]\n",
      "  [0.99215686 1.         0.95686275]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.         0.9843137  0.92941177]\n",
      "  [1.         0.9843137  0.92941177]\n",
      "  [1.         0.9843137  0.92941177]\n",
      "  ...\n",
      "  [0.20784314 0.24313726 0.12941177]\n",
      "  [0.21568628 0.23137255 0.16470589]\n",
      "  [0.20784314 0.23137255 0.18431373]]\n",
      "\n",
      " [[1.         1.         0.9098039 ]\n",
      "  [1.         1.         0.9098039 ]\n",
      "  [1.         1.         0.91764706]\n",
      "  ...\n",
      "  [0.30588236 0.18431373 0.17254902]\n",
      "  [0.29803923 0.18431373 0.20392157]\n",
      "  [0.2784314  0.1882353  0.22745098]]\n",
      "\n",
      " [[0.98039216 1.         0.9019608 ]\n",
      "  [0.98039216 1.         0.9019608 ]\n",
      "  [0.98039216 1.         0.9098039 ]\n",
      "  ...\n",
      "  [0.42352942 0.11764706 0.21176471]\n",
      "  [0.4        0.12156863 0.24705882]\n",
      "  [0.37254903 0.13333334 0.26666668]]]\n",
      "y_train[0] 3\n",
      "Shapes x_train, y_train (100000, 32, 64, 3) (100000,)\n",
      "Shapes x_test (20000, 32, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "styles_encoder = LabelEncoder().fit(train_data['style_name'].unique()) # use this for sparse format not one-hot\n",
    "y_train =  styles_encoder.transform(train_data['style_name'])\n",
    "\n",
    "X, X_cv, y, y_cv = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size = 0.05,\n",
    "    random_state = 15\n",
    ")\n",
    "\n",
    "print(\"training label distribution\", Counter(y))\n",
    "print(\"validation label distribution\", Counter(y_cv))\n",
    "\n",
    "print(\"X_train[0]\", X_train[0])\n",
    "print(\"y_train[0]\",y_train[0])\n",
    "print(\"Shapes x_train, y_train\", X_train.shape, y_train.shape) # 500, 32, 64, 3\n",
    "print(\"Shapes x_test\", X_test.shape) # 500, 32, 64, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(40, kernel_size=5, padding=\"same\",input_shape=(32, 64, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# 2nd convo layer - increase feature map\n",
    "model.add(Conv2D(70, kernel_size=3, padding=\"same\", activation = 'relu'))\n",
    "model.add(Conv2D(200, kernel_size=3, padding=\"same\", activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=3, padding=\"valid\", activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "# Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',\n",
    "#                 input_shape=[32, 64, 3]))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# # add 2nd conv layer\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#             optimizer=keras.optimizers.Adadelta(),\n",
    "#             metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 32, 16)   1200        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 32, 16)   64          conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 32, 16)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 32, 16)   6400        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 32, 16)   64          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 32, 16)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 32, 16)   64          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 32, 16)   0           activation_27[0][0]              \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 32, 16)   64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 32, 16)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 16, 32)    4608        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 16, 32)    128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 16, 32)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 16, 32)    9216        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 16, 32)    128         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 16, 32)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 16, 32)    544         activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 16, 32)    0           activation_30[0][0]              \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 16, 32)    128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 16, 32)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 8, 32)     9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 8, 32)     128         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 8, 32)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 8, 32)     9216        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 8, 32)     128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 8, 32)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 4, 8, 32)     1056        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 8, 32)     0           activation_33[0][0]              \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 8, 32)     128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 8, 32)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 2, 4, 32)     9216        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 2, 4, 32)     128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2, 4, 32)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 2, 4, 32)     9216        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2, 4, 32)     128         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 2, 4, 32)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 2, 4, 32)     1056        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 2, 4, 32)     0           activation_36[0][0]              \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2, 4, 32)     128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 2, 4, 32)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16384       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64)           256         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 64)           0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           650         activation_38[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 79,642\n",
      "Trainable params: 78,842\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "inputs = Input(shape=(32, 64, 3))\n",
    "\n",
    "def conv_block(inp, filters, kernel_size, strides):\n",
    "    block = Conv2D(filters=filters, kernel_size=kernel_size, \n",
    "                   strides=strides, padding='same', \n",
    "                   kernel_initializer='he_normal', activation=None, \n",
    "                   use_bias=False)(inp)\n",
    "    block = BatchNormalization()(block)\n",
    "    return Activation('relu')(block)\n",
    "\n",
    "def res_net_block(inp, filters, kernel_size=3):\n",
    "    # main path is two conv blocks with first reducing spatial size using striding\n",
    "    main_path = conv_block(inp, filters, kernel_size, strides=2)    \n",
    "    main_path = conv_block(main_path, filters, kernel_size, strides=1)\n",
    "    #main_path = conv_block(main_path, filters, kernel_size, strides=1)\n",
    "    \n",
    "    # residual path is a linear projection with stride=2 to do equivalent\n",
    "    # spatial reduction\n",
    "    residual_path = Conv2D(filters=filters, kernel_size=1,\n",
    "                           strides=2, padding='same', activation=None)(inp) \n",
    "    \n",
    "    # TODO: review combos of BN positioning from \"Identity mappings in deep residual networks\"\n",
    "    model = Add()([main_path, residual_path])\n",
    "    model = BatchNormalization()(model)\n",
    "    return Activation('relu')(model)\n",
    "\n",
    "resnet = res_net_block(inputs, filters=16, kernel_size=5)\n",
    "resnet = res_net_block(resnet, filters=32)\n",
    "resnet = res_net_block(resnet, filters=32)\n",
    "resnet = res_net_block(resnet, filters=32)\n",
    "\n",
    "mlp = Flatten()(resnet)\n",
    "mlp = Dropout(rate=0.5)(mlp)\n",
    "\n",
    "mlp = Dense(units=64, activation=None, use_bias=False)(mlp)\n",
    "mlp = BatchNormalization()(mlp)\n",
    "mlp = Activation('relu')(mlp)\n",
    "\n",
    "predictions = Dense(units=10, activation='softmax')(mlp)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizers.Adam(lr=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "Train on 99000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "99000/99000 [==============================] - 148s 1ms/sample - loss: 0.0753 - acc: 0.9698 - val_loss: 0.1203 - val_acc: 0.9590\n",
      "Epoch 2/3\n",
      " 1536/99000 [..............................] - ETA: 2:20 - loss: 0.0699 - acc: 0.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/tw_img_clf-KchzMM_2/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         validation_data=(X_cv, y_cv))\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tw_img_clf-KchzMM_2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tw_img_clf-KchzMM_2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tw_img_clf-KchzMM_2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tw_img_clf-KchzMM_2/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "EPOCHS=3\n",
    "BATCH_SIZE=512\n",
    "\n",
    "X_train, X_test = normalise(train_images), normalise(test_images)\n",
    "X, X_cv, y, y_cv = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size = 0.01,\n",
    "    random_state = 15\n",
    ")\n",
    "print(\"Training starting...\")\n",
    "\n",
    "model.fit(X, y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=(X_cv, y_cv))\n",
    "score = model.evaluate(X_cv, y_cv, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.972549  , 1.        , 0.7764706 ],\n",
       "        [1.        , 1.        , 0.8509804 ],\n",
       "        [1.        , 0.9019608 , 1.        ],\n",
       "        ...,\n",
       "        [0.7411765 , 0.42352942, 0.4509804 ],\n",
       "        [0.67058825, 0.4627451 , 0.43137255],\n",
       "        [0.60784316, 0.5019608 , 0.3882353 ]],\n",
       "\n",
       "       [[0.9607843 , 1.        , 0.8117647 ],\n",
       "        [1.        , 1.        , 0.8901961 ],\n",
       "        [1.        , 0.91764706, 1.        ],\n",
       "        ...,\n",
       "        [0.65882355, 0.4627451 , 0.46666667],\n",
       "        [0.654902  , 0.47058824, 0.4392157 ],\n",
       "        [0.6627451 , 0.47058824, 0.40392157]],\n",
       "\n",
       "       [[0.9843137 , 1.        , 0.89411765],\n",
       "        [0.99607843, 1.        , 0.91764706],\n",
       "        [0.95686275, 0.8156863 , 0.9882353 ],\n",
       "        ...,\n",
       "        [0.6431373 , 0.46666667, 0.47843137],\n",
       "        [0.7176471 , 0.43137255, 0.45882353],\n",
       "        [0.8117647 , 0.39215687, 0.41568628]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.99215686, 1.        , 0.92941177],\n",
       "        [0.99215686, 1.        , 0.92941177],\n",
       "        [0.99215686, 1.        , 0.92941177],\n",
       "        ...,\n",
       "        [0.3019608 , 0.99607843, 0.4862745 ],\n",
       "        [0.3372549 , 0.99215686, 0.39607844],\n",
       "        [0.36078432, 0.99215686, 0.34117648]],\n",
       "\n",
       "       [[0.9882353 , 1.        , 0.92941177],\n",
       "        [0.9882353 , 1.        , 0.92941177],\n",
       "        [0.9882353 , 1.        , 0.92941177],\n",
       "        ...,\n",
       "        [0.3372549 , 0.98039216, 0.46666667],\n",
       "        [0.3764706 , 0.98039216, 0.36078432],\n",
       "        [0.40392157, 0.98039216, 0.2901961 ]],\n",
       "\n",
       "       [[0.9882353 , 1.        , 0.92941177],\n",
       "        [0.9882353 , 1.        , 0.92941177],\n",
       "        [0.9882353 , 1.        , 0.92941177],\n",
       "        ...,\n",
       "        [0.36078432, 0.972549  , 0.44313726],\n",
       "        [0.40392157, 0.96862745, 0.34117648],\n",
       "        [0.43137255, 0.96862745, 0.27058825]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/23 [===============================] - 3s 133ms/step - loss: 0.1743 - acc: 0.9349 - val_loss: 116.6425 - val_acc: 0.2344\n",
      "Epoch 2/20\n",
      "24/23 [===============================] - 3s 128ms/step - loss: 0.1562 - acc: 0.9310 - val_loss: 33.3861 - val_acc: 0.3516\n",
      "Epoch 3/20\n",
      "24/23 [===============================] - 3s 131ms/step - loss: 0.1798 - acc: 0.9329 - val_loss: 7.9898 - val_acc: 0.6016\n",
      "Epoch 4/20\n",
      "24/23 [===============================] - 3s 135ms/step - loss: 0.1645 - acc: 0.9284 - val_loss: 3.1891 - val_acc: 0.8281\n",
      "Epoch 5/20\n",
      "24/23 [===============================] - 3s 130ms/step - loss: 0.1585 - acc: 0.9375 - val_loss: 1.8164 - val_acc: 0.8438\n",
      "Epoch 6/20\n",
      "24/23 [===============================] - 3s 134ms/step - loss: 0.1658 - acc: 0.9323 - val_loss: 1.1751 - val_acc: 0.8359\n",
      "Epoch 7/20\n",
      "24/23 [===============================] - 3s 130ms/step - loss: 0.1657 - acc: 0.9368 - val_loss: 0.6347 - val_acc: 0.8828\n",
      "Epoch 8/20\n",
      "24/23 [===============================] - 3s 135ms/step - loss: 0.1848 - acc: 0.9264 - val_loss: 0.5258 - val_acc: 0.8984\n",
      "Epoch 9/20\n",
      "24/23 [===============================] - 3s 132ms/step - loss: 0.1569 - acc: 0.9395 - val_loss: 2.8566 - val_acc: 0.7031\n",
      "Epoch 10/20\n",
      "24/23 [===============================] - 3s 134ms/step - loss: 0.1582 - acc: 0.9395 - val_loss: 1.5073 - val_acc: 0.7422\n",
      "Epoch 11/20\n",
      "24/23 [===============================] - 3s 136ms/step - loss: 0.1606 - acc: 0.9349 - val_loss: 0.4084 - val_acc: 0.9141\n",
      "Epoch 12/20\n",
      "24/23 [===============================] - 3s 135ms/step - loss: 0.1540 - acc: 0.9395 - val_loss: 0.3478 - val_acc: 0.9375\n",
      "Epoch 13/20\n",
      "24/23 [===============================] - 3s 138ms/step - loss: 0.1392 - acc: 0.9408 - val_loss: 0.2725 - val_acc: 0.9375\n",
      "Epoch 14/20\n",
      "24/23 [===============================] - 3s 137ms/step - loss: 0.1543 - acc: 0.9375 - val_loss: 0.2717 - val_acc: 0.9297\n",
      "Epoch 15/20\n",
      "24/23 [===============================] - 3s 134ms/step - loss: 0.1829 - acc: 0.9277 - val_loss: 0.3264 - val_acc: 0.9219\n",
      "Epoch 16/20\n",
      "24/23 [===============================] - 3s 138ms/step - loss: 0.1631 - acc: 0.9303 - val_loss: 0.2732 - val_acc: 0.9375\n",
      "Epoch 17/20\n",
      "24/23 [===============================] - 3s 134ms/step - loss: 0.1657 - acc: 0.9362 - val_loss: 0.2143 - val_acc: 0.9531\n",
      "Epoch 18/20\n",
      "24/23 [===============================] - 3s 139ms/step - loss: 0.1412 - acc: 0.9440 - val_loss: 0.2688 - val_acc: 0.9531\n",
      "Epoch 19/20\n",
      "24/23 [===============================] - 3s 138ms/step - loss: 0.1728 - acc: 0.9297 - val_loss: 0.2676 - val_acc: 0.9531\n",
      "Epoch 20/20\n",
      "24/23 [===============================] - 3s 134ms/step - loss: 0.1613 - acc: 0.9427 - val_loss: 0.2442 - val_acc: 0.9453\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "X_train = X_train.reshape(-1,32,64,3)\n",
    "X_test = X_test.reshape(-1,32,64,3)\n",
    "\n",
    "X_train, X_test = normalise(train_images), normalise(test_images)\n",
    "X, X_cv, y, y_cv = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size = 0.05,\n",
    "    random_state = 15\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    fill_mode='nearest',\n",
    "    validation_split = 0.05\n",
    "    )\n",
    "\n",
    "# generator = datagen.flow_from_directory(\n",
    "#         'data/test',\n",
    "#         target_size=(150, 150),\n",
    "#         batch_size=16,\n",
    "#         class_mode=None,  # only data, no labels\n",
    "#         shuffle=False)  # keep data in same order as labels\n",
    "\n",
    "\n",
    "datagen.fit(X_train)\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, subset='training')\n",
    "validation_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, subset='validation')\n",
    "\n",
    "\n",
    "# # fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    steps_per_epoch = len(train_generator) / BATCH_SIZE,\n",
    "                    validation_steps = len(validation_generator) / BATCH_SIZE,\n",
    "                    epochs = 20,\n",
    "                    workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen_test = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=10,\n",
    "#     fill_mode='nearest',\n",
    "#     validation_split = 0.05\n",
    "# )\n",
    "\n",
    "# # generator = datagen.flow_from_directory(\n",
    "# #         'data/test',\n",
    "# #         target_size=(150, 150),\n",
    "# #         batch_size=16,\n",
    "# #         class_mode=None,  # only data, no labels\n",
    "# #         shuffle=False)  # keep data in same order as labels\n",
    "\n",
    "# datagen_test.fit(X_test)\n",
    "# test_generator = datagen_test.flow(X_test)\n",
    "# preds = model.predict_generator(test_generator)\n",
    "\n",
    "X_test =normalise(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({8: 1985,\n",
       "         5: 2072,\n",
       "         7: 1976,\n",
       "         2: 2067,\n",
       "         3: 2000,\n",
       "         0: 2163,\n",
       "         6: 1984,\n",
       "         9: 2019,\n",
       "         4: 2000,\n",
       "         1: 1734})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "# preds = model.predict(X_test_std)\n",
    "\n",
    "test_data['label_id'] = np.argmax(preds, axis = 1)\n",
    "Counter(test_data['label_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Beijing': 2163, 'Melbourne': 2072, 'Geneva': 2067, 'Zurich': 2019, 'HongKong': 2000, 'Luanda': 2000, 'Sydney': 1985, 'Seoul': 1984, 'Singapore': 1976, 'Brisbane': 1734})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(            id                file                          filepath  \\\n",
       " 0      9000000  test-A-9000000.jpg  synimg/test/A/test-A-9000000.jpg   \n",
       " 1      9000001  test-B-9000001.jpg  synimg/test/B/test-B-9000001.jpg   \n",
       " 2      9000002  test-C-9000002.jpg  synimg/test/C/test-C-9000002.jpg   \n",
       " 3      9000003  test-D-9000003.jpg  synimg/test/D/test-D-9000003.jpg   \n",
       " 4      9000004  test-E-9000004.jpg  synimg/test/E/test-E-9000004.jpg   \n",
       " ...        ...                 ...                               ...   \n",
       " 19995  9019995  test-F-9019995.jpg  synimg/test/F/test-F-9019995.jpg   \n",
       " 19996  9019996  test-G-9019996.jpg  synimg/test/G/test-G-9019996.jpg   \n",
       " 19997  9019997  test-H-9019997.jpg  synimg/test/H/test-H-9019997.jpg   \n",
       " 19998  9019998  test-I-9019998.jpg  synimg/test/I/test-I-9019998.jpg   \n",
       " 19999  9019999  test-J-9019999.jpg  synimg/test/J/test-J-9019999.jpg   \n",
       " \n",
       "        label_id style_name  \n",
       " 0             8     Sydney  \n",
       " 1             5  Melbourne  \n",
       " 2             5  Melbourne  \n",
       " 3             7  Singapore  \n",
       " 4             2     Geneva  \n",
       " ...         ...        ...  \n",
       " 19995         7  Singapore  \n",
       " 19996         1   Brisbane  \n",
       " 19997         7  Singapore  \n",
       " 19998         4     Luanda  \n",
       " 19999         3   HongKong  \n",
       " \n",
       " [20000 rows x 5 columns],\n",
       " Counter({'Sydney': 1985,\n",
       "          'Melbourne': 2072,\n",
       "          'Singapore': 1976,\n",
       "          'Geneva': 2067,\n",
       "          'HongKong': 2000,\n",
       "          'Beijing': 2163,\n",
       "          'Seoul': 1984,\n",
       "          'Zurich': 2019,\n",
       "          'Luanda': 2000,\n",
       "          'Brisbane': 1734}))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import write_output\n",
    "write_output(test_data, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
